{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # para movernos en carpeta dentro del S.O\n",
    "import os  # movernos por carpetas|\n",
    "import numpy as np #generación y manejo de datos\n",
    "from keras.utils import plot_model #genérica de gráfica \n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator #Para tratamiento de imagenes\n",
    "from tensorflow.keras import optimizers #obtener optimizador\n",
    "from tensorflow.keras.models import Sequential #redes neuronales secuenciales, capas en orden\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Activation\n",
    "from tensorflow.keras.layers import  Convolution2D, MaxPooling2D \n",
    "from tensorflow.keras.callbacks import  TensorBoard ##Plataforma para visualizar las graficas\n",
    "import tensorflow as tf #cálculo numérico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Cargamos nuestros Data\n",
    "data_entrenamiento = './Plagas_Arroz/train'\n",
    "data_validacion = './Plagas_Arroz/test'\n",
    "\n",
    "###Parametros de la red neuronal\n",
    "epocas = 10 #cantidad de iteracion\n",
    "altura,longitud = 150 , 150 # tamaño de las imagenes pixeles\n",
    "batch_size = 32 #numero de imagenes a procesar en cada paso\n",
    "pasos = 1000   #numero de veces q se procesa la informacion en las epocas\n",
    "filtrosConv1 = 32 #numero de filtro\n",
    "filtrosConv2 = 64 #numero de filtro\n",
    "filtrosConv3 = 128 #numero de filtro\n",
    "filtrosConv4 = 256 #numero de filtro\n",
    "tamano_filtro1 = (3, 3) #Kernel \n",
    "tamano_filtro2 = (2, 2) #Kernel \n",
    "tamano_filtro3 = (2, 2) #Kernel \n",
    "tamano_filtro4 = (2, 2) #Kernel \n",
    "tamano_pool = (2, 2) #tamaño del filtro del Maxpoling\n",
    "clases = 9  #numero de clases\n",
    "lr = 0.001 #learning rate ajuste de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Pre_Procesamiento de Imagenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Prepocesamiento de iamgenes para luego usarlas en la red \n",
    "\n",
    "#Entrenamiento\n",
    "#Para que haya mejor entrenamiento en la imagenes\n",
    "entrenamiento_datagen = ImageDataGenerator( \n",
    "    rescale= 1. / 255,     #todos lo pixels poseen rango de 0-255 #Reescalado cada pixel de 0 a 1 mejor eficiencia\n",
    "    shear_range=0.2,       #genera imagenes en inclinacion\n",
    "    zoom_range=0.2,        #zoom a las imagenes\n",
    "    horizontal_flip=True)  #invertir imagenes \n",
    "\n",
    "##Validacion\n",
    "#para la validacion solo reescalamos con el fin iran las imagenes como son \n",
    "validacion_datagen = ImageDataGenerator(\n",
    "    rescale= 1. / 255)      #cada pixel de 0 a 1 mejor eficiencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2700 images belonging to 9 classes.\n",
      "Found 450 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "#entra al directorio y procesa imagenes segun los datos dados\n",
    "imagen_entrenamiento = entrenamiento_datagen.flow_from_directory(\n",
    "    data_entrenamiento,## se extrae imagenes \n",
    "    target_size=(altura, longitud), #todas la iamgenes a una altura y longitud dada\n",
    "    batch_size=batch_size,  #se procesa con un batch size de 32 \n",
    "    class_mode='categorical',)##se usa porque la clasificacion es por categoria\n",
    "\n",
    "##Validacion\n",
    "imagen_validacion= validacion_datagen .flow_from_directory(\n",
    "    data_validacion,\n",
    "    target_size=(altura, longitud),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Ejemplo de una plaga normalizada\n",
    "from PIL import Image\n",
    "im = Image.open('normalizada.jpg')\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###CReacion de Red convolucion \n",
    "\n",
    "##Estructura de la red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estructura de la REd\n",
    "\n",
    "model = Sequential() ##Red secuencial, es decir varias capas apiladas\n",
    "\n",
    "##Primera capa con 32 filtro, 3 canales rgb\n",
    "#SAME  la salida tenga el mismo tamaño que la entrada\n",
    "#Relu anula los valores negativos a positivo\n",
    "##«2×2» (2 de alto por 2 de ancho = 4 pixeles)\n",
    "\n",
    "##Capa de entrada\n",
    "model.add(Convolution2D(filtrosConv1, tamano_filtro1, padding =\"same\", input_shape=(longitud, altura, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "\n",
    "\n",
    "##Capa Oculta\n",
    "##segunda capa con 64 filtro\n",
    "model.add(Convolution2D(filtrosConv2, tamano_filtro2, padding =\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "\n",
    "\n",
    "##Tercera capa con 128 filtro\n",
    "model.add(Convolution2D(filtrosConv3, tamano_filtro3, padding =\"same\", activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=tamano_pool))\n",
    "\n",
    "\n",
    "model.add(Flatten())  ##iamgen se aplana y devuelve una matriz unidimensional\n",
    "##256 neuronas es la topologia de la red\n",
    "model.add(Dense(256, activation='relu'))  #capa dense para conectar 256 neuronas en una sola capa\n",
    "model.add(Dropout(0.5))     ##apagar neuronas a la capa dense el 50% evitar sobre ajuste\n",
    "\n",
    "\n",
    "############Capa de salida\n",
    "model.add(Dense(clases, activation='softmax')) #9 neuronas, sotfmax obtener probabilidades \n",
    "\n",
    "#'categorical_crossentropy' para ver q tambien o q mal ira \n",
    "model.compile(loss='categorical_crossentropy', ## parametro para optimizar el algoritmo\n",
    "            optimizer=optimizers.Adam(lr), ##optimizador Adam\n",
    "            metrics=['accuracy'])    ## porcentaje de como aprende nuestra red neuronal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##entrenar algoritmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "49/84 [================>.............] - ETA: 15s - loss: 2.2462 - accuracy: 0.1728"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:945: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 47s 553ms/step - loss: 2.1456 - accuracy: 0.2083 - val_loss: 1.7278 - val_accuracy: 0.3772\n",
      "Epoch 2/10\n",
      "84/84 [==============================] - 47s 561ms/step - loss: 1.6710 - accuracy: 0.3966 - val_loss: 1.5075 - val_accuracy: 0.4866\n",
      "Epoch 3/10\n",
      "84/84 [==============================] - 39s 468ms/step - loss: 1.3300 - accuracy: 0.5337 - val_loss: 1.0873 - val_accuracy: 0.6496\n",
      "Epoch 4/10\n",
      "84/84 [==============================] - 38s 447ms/step - loss: 1.0437 - accuracy: 0.6409 - val_loss: 0.8105 - val_accuracy: 0.7589\n",
      "Epoch 5/10\n",
      "84/84 [==============================] - 40s 472ms/step - loss: 0.8768 - accuracy: 0.7121 - val_loss: 0.7528 - val_accuracy: 0.7946\n",
      "Epoch 6/10\n",
      "84/84 [==============================] - 40s 472ms/step - loss: 0.7079 - accuracy: 0.7654 - val_loss: 0.6203 - val_accuracy: 0.8304\n",
      "Epoch 7/10\n",
      "84/84 [==============================] - 40s 479ms/step - loss: 0.5785 - accuracy: 0.8047 - val_loss: 0.5050 - val_accuracy: 0.8638\n",
      "Epoch 8/10\n",
      "84/84 [==============================] - 38s 451ms/step - loss: 0.4427 - accuracy: 0.8561 - val_loss: 0.3483 - val_accuracy: 0.9174\n",
      "Epoch 9/10\n",
      "84/84 [==============================] - 38s 454ms/step - loss: 0.3712 - accuracy: 0.8793 - val_loss: 0.3919 - val_accuracy: 0.9107\n",
      "Epoch 10/10\n",
      "84/84 [==============================] - 41s 480ms/step - loss: 0.3159 - accuracy: 0.9022 - val_loss: 0.2600 - val_accuracy: 0.9308\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a62e9177f0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#entrenar red neruonal con imagenes de entrenamiento\n",
    "model.fit(imagen_entrenamiento,\n",
    "    epochs=epocas,\n",
    "    validation_data = imagen_validacion,\n",
    "    steps_per_epoch = imagen_entrenamiento.n//batch_size, #se optiene los pasos mas optimo, 84 pasos\n",
    "    validation_steps = imagen_validacion.n //batch_size,   #se procesa 32 imagenes por paso, 84 pasos\n",
    "    callbacks = [TensorBoard (log_dir = 'logs/Grafica#18')],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 150, 150, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 75, 75, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 75, 75, 64)        8256      \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 37, 37, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 37, 37, 128)       32896     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 18, 18, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 41472)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               10617088  \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 9)                 2313      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,661,449\n",
      "Trainable params: 10,661,449\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#obtner informacion de la red neuronal\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" tf.keras.utils.plot_model(model,\\n                          to_file='nombre-nn.png',\\n                          show_shapes=False,\\n                          show_dtype=False,\\n                          show_layer_names=True,\\n                          rankdir='TB',\\n                          expand_nested=False,\\n                          dpi=96,\\n                          layer_range=None,\\n                          show_layer_activations=True) \""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sirve para poder graficar la estructura de la red\n",
    "tf.keras.utils.plot_model(model,\n",
    "                          to_file='nombre-nn.png',\n",
    "                          show_shapes=False,\n",
    "                          show_dtype=False,\n",
    "                          show_layer_names=True,\n",
    "                          rankdir='TB',\n",
    "                          expand_nested=False,\n",
    "                          dpi=96,\n",
    "                          layer_range=None,\n",
    "                          show_layer_activations=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Guardar modelo en un archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Modelo/modelo.P52\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Modelo/modelo.P52\\assets\n"
     ]
    }
   ],
   "source": [
    "###Se guarda el modelo \n",
    "target_dir = './Modelo/'\n",
    "if not os.path.exists(target_dir):\n",
    "  os.mkdir(target_dir)\n",
    "model.save('./Modelo/modelo.P52')\n",
    "model.save_weights('./Modelo/pesos.P52')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27f6fea6f47ae512550f0b8facdbd035a93e1dd89633f7bf2dd00a2502c71d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
